{"cells":[{"cell_type":"markdown","metadata":{"id":"jiCl_6Y6wbuZ"},"source":["### Practical no.: 4\n","Name: Madhav Jha \u003cbr\u003e\n","Roll no.: 48 \u003cbr\u003e\n","Batch: E3 \u003cbr\u003e\n","Branch: CSE(AIML) \u003cbr\u003e\n","Semester: VI"]},{"cell_type":"markdown","metadata":{"id":"Kq6vcxI9wc-D"},"source":["### AIM:\n","### 1. Perform morphological analysis and lemmatization on the text corpora. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yfgwrk-gqf9z"},"outputs":[],"source":["# !pip install nltk\n","import nltk\n","# nltk.download('all')\n","from nltk.book import *"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447,"status":"ok","timestamp":1681106598415,"user":{"displayName":"E-48 Madhav Jha","userId":"17793726210268974405"},"user_tz":-330},"id":"vg66fH3Bqbdo","outputId":"4b2a6b0e-1eea-45ee-978b-dc7e514f5811"},"outputs":[{"name":"stdout","output_type":"stream","text":["['dancing', 'wait', 'waiting', 'waited', 'waits', 'Study', 'Studied', 'Studying', 'Learn', 'Learned', 'Learning', 'sincerely', 'electricity', 'roughly', 'ringing']\n"]}],"source":["words = [\"dancing\",\"wait\", \"waiting\", \"waited\", \"waits\",\n","          \"Study\",\"Studied\",\"Studying\",\n","          \"Learn\",\"Learned\",\"Learning\",'sincerely','electricity','roughly','ringing']\n","print(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFbEuUKJ9kC6"},"outputs":[],"source":["import pandas as pd\n","dic = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680502785517,"user":{"displayName":"E-48 Madhav Jha","userId":"17793726210268974405"},"user_tz":-330},"id":"Ke8tobe2qmEa","outputId":"779ddaa4-41d9-48b7-9e75-c41da41610c3"},"outputs":[{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePorterStemmer\u003c/th\u003e\n","      \u003cth\u003eSnowballStemmer\u003c/th\u003e\n","      \u003cth\u003eLancasterStemmer\u003c/th\u003e\n","      \u003cth\u003eRegexpStemmer\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003edancing\u003c/th\u003e\n","      \u003ctd\u003edanc\u003c/td\u003e\n","      \u003ctd\u003edanc\u003c/td\u003e\n","      \u003ctd\u003edanc\u003c/td\u003e\n","      \u003ctd\u003edanc\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003ewait\u003c/th\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003ewaiting\u003c/th\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003ewaited\u003c/th\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003ewaits\u003c/th\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","      \u003ctd\u003ewait\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eStudy\u003c/th\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eStudied\u003c/th\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eStudying\u003c/th\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","      \u003ctd\u003estudi\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eLearn\u003c/th\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eLearned\u003c/th\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eLearning\u003c/th\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","      \u003ctd\u003elearn\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003esincerely\u003c/th\u003e\n","      \u003ctd\u003esincer\u003c/td\u003e\n","      \u003ctd\u003esincer\u003c/td\u003e\n","      \u003ctd\u003esincer\u003c/td\u003e\n","      \u003ctd\u003esincer\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eelectricity\u003c/th\u003e\n","      \u003ctd\u003eelectr\u003c/td\u003e\n","      \u003ctd\u003eelectr\u003c/td\u003e\n","      \u003ctd\u003eelectr\u003c/td\u003e\n","      \u003ctd\u003eelectr\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eroughly\u003c/th\u003e\n","      \u003ctd\u003eroughli\u003c/td\u003e\n","      \u003ctd\u003eroughli\u003c/td\u003e\n","      \u003ctd\u003eroughli\u003c/td\u003e\n","      \u003ctd\u003eroughli\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eringing\u003c/th\u003e\n","      \u003ctd\u003ering\u003c/td\u003e\n","      \u003ctd\u003ering\u003c/td\u003e\n","      \u003ctd\u003ering\u003c/td\u003e\n","      \u003ctd\u003ering\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["            PorterStemmer SnowballStemmer LancasterStemmer RegexpStemmer\n","dancing              danc            danc             danc          danc\n","wait                 wait            wait             wait          wait\n","waiting              wait            wait             wait          wait\n","waited               wait            wait             wait          wait\n","waits                wait            wait             wait          wait\n","Study               studi           studi            studi         studi\n","Studied             studi           studi            studi         studi\n","Studying            studi           studi            studi         studi\n","Learn               learn           learn            learn         learn\n","Learned             learn           learn            learn         learn\n","Learning            learn           learn            learn         learn\n","sincerely          sincer          sincer           sincer        sincer\n","electricity        electr          electr           electr        electr\n","roughly           roughli         roughli          roughli       roughli\n","ringing              ring            ring             ring          ring"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.stem import PorterStemmer\n","from nltk.stem import SnowballStemmer \n","from nltk.stem import LancasterStemmer \n","from nltk.stem import RegexpStemmer \n","\n","\n","# porter stemmer.\n","stemmer_ps = PorterStemmer()  \n","stemmed_words_ps = [stemmer_ps.stem(word) for word in words]\n","# print(\"Porter stemmed words: \", stemmed_words_ps)\n","dic[\"PorterStemmer\"] = stemmed_words_ps\n","\n","# Snowball Stemmer\n","stemmer_ss = SnowballStemmer(\"english\")   \n","stemmed_words_ss = [stemmer_ss.stem(word) for word in words]\n","# print(\"Snowball stemmed words: \", stemmed_words_ss)\n","dic[\"SnowballStemmer\"] = stemmed_words_ps\n","\n","# Lancaster Stemmer\n","stemmer_ss = LancasterStemmer()   \n","stemmed_words_ss = [stemmer_ss.stem(word) for word in words]\n","# print(\"Lancaster stemmed words: \", stemmed_words_ss)\n","dic[\"LancasterStemmer\"] = stemmed_words_ps\n","\n","# Regexp Stemmer\n","stemmer_ss = RegexpStemmer(\"english\")   \n","stemmed_words_ss = [stemmer_ss.stem(word) for word in words]\n","# print(\"Regexp stemmed words: \", stemmed_words_ss)\n","dic[\"RegexpStemmer\"] = stemmed_words_ps\n","\n","# print(dic.keys())\n","col = []\n","for i in dic.keys():\n","    col.append(i)\n","\n","df = pd.DataFrame(dic,words,columns=col)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680504281292,"user":{"displayName":"E-48 Madhav Jha","userId":"17793726210268974405"},"user_tz":-330},"id":"cwCsLNbGqxXt","outputId":"8faa657a-1312-4c96-e019-d891f35792dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["The lemmatized words:  ['dancing', 'wait', 'waiting', 'waited', 'wait', 'Study', 'Studied', 'Studying', 'Learn', 'Learned', 'Learning', 'sincerely', 'electricity', 'roughly', 'ringing']\n","The lemmatized words using a POS tag:  ['dance', 'wait', 'wait', 'wait', 'wait', 'Study', 'Studied', 'Studying', 'Learn', 'Learned', 'Learning', 'sincerely', 'electricity', 'roughly', 'ring']\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","\n","sample_words = words\n","\n","def lem(words):\n","    lemmatizer = WordNetLemmatizer()   \n","\n","    #an instance of Word Net Lemmatizer\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words] \n","    print(\"The lemmatized words: \", lemmatized_words)\n","\n","    #prints the lemmatized words\n","    lemmatized_words_pos = [lemmatizer.lemmatize(word, pos = \"v\") for word in words]\n","    print(\"The lemmatized words using a POS tag: \", lemmatized_words_pos) \n","\n","lem(sample_words)"]},{"cell_type":"markdown","metadata":{"id":"SH4uXMW7vY6F"},"source":["### Lemmatization using spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWlqMkT6vYkN"},"outputs":[],"source":["!pip install spacy"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1756,"status":"ok","timestamp":1681106500942,"user":{"displayName":"E-48 Madhav Jha","userId":"17793726210268974405"},"user_tz":-330},"id":"xny8qmwQvqYG"},"outputs":[],"source":["import spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LmbH_DH_v6xp"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-05-22 18:14:39.821049: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-22 18:14:41.204090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy\u003c3.6.0,\u003e=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n","Requirement already satisfied: spacy-legacy\u003c3.1.0,\u003e=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: thinc\u003c8.2.0,\u003e=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (8.1.9)\n","Requirement already satisfied: wasabi\u003c1.2.0,\u003e=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (1.1.1)\n","Requirement already satisfied: srsly\u003c3.0.0,\u003e=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: catalogue\u003c2.1.0,\u003e=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: typer\u003c0.8.0,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: pathy\u003e=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: smart-open\u003c7.0.0,\u003e=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (1.22.4)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (2.27.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,\u003c1.11.0,\u003e=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (1.10.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (67.7.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (23.1)\n","Requirement already satisfied: langcodes\u003c4.0.0,\u003e=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: typing-extensions\u003e=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c1.11.0,\u003e=1.7.4-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (4.5.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (3.4)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc\u003c8.2.0,\u003e=8.1.8-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: confection\u003c1.0.0,\u003e=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc\u003c8.2.0,\u003e=8.1.8-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: click\u003c9.0.0,\u003e=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer\u003c0.8.0,\u003e=0.3.0-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003espacy\u003c3.6.0,\u003e=3.5.0-\u003een-core-web-sm==3.5.0) (2.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!python3 -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1236,"status":"ok","timestamp":1681106577727,"user":{"displayName":"E-48 Madhav Jha","userId":"17793726210268974405"},"user_tz":-330},"id":"KqdCBEYTvvaG"},"outputs":[],"source":["load_model = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":616,"status":"ok","timestamp":1681106637002,"user":{"displayName":"E-48 Madhav Jha","userId":"17793726210268974405"},"user_tz":-330},"id":"kKM_HGnGwtQQ","outputId":"167f9a90-1d36-482f-c881-aa3c74285bd8"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'dancing wait waiting waited waits Study Studied Studying Learn Learned Learning sincerely electricity roughly ringing'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tempWords = \" \".join(words)\n","tempWords"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":426,"status":"ok","timestamp":1681106663164,"user":{"displayName":"E-48 Madhav Jha","userId":"17793726210268974405"},"user_tz":-330},"id":"NWPRAx93wrwk","outputId":"0f4e8826-bacd-44aa-899d-7320ab919802"},"outputs":[{"data":{"text/plain":["['dance',\n"," 'wait',\n"," 'wait',\n"," 'wait',\n"," 'wait',\n"," 'Study',\n"," 'Studied',\n"," 'Studying',\n"," 'Learn',\n"," 'Learned',\n"," 'Learning',\n"," 'sincerely',\n"," 'electricity',\n"," 'roughly',\n"," 'ring']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["doc = load_model(tempWords)\n","[w.lemma_ for w in doc]"]},{"cell_type":"markdown","metadata":{"id":"VgWWxNIEqXO8"},"source":["### 2. Detect and Extract specific entities from the text using NER(Named Entity Recognition)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2hWw_b1wZbQ"},"outputs":[],"source":["from nltk.corpus import state_union\n","from nltk.tokenize import PunktSentenceTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CV5B-Efjvw4z"},"outputs":[],"source":["# process the text and print Named entities\n","# tokenization\n","train_text = state_union.raw()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9U_KmnVvw41"},"outputs":[],"source":["sample_text = state_union.raw(\"2006-GWBush.txt\")\n","custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n","tokenized = custom_sent_tokenizer.tokenize(sample_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ci81wDMlvw43","outputId":"cdc456af-1974-49bc-f55d-8090db73b05a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(S\n","  PRESIDENT/NNP\n","  (PERSON GEORGE/NNP W./NNP BUSH/NNP)\n","  'S/POS\n","  (ORGANIZATION ADDRESS/NNP)\n","  BEFORE/IN\n","  A/NNP\n","  (ORGANIZATION JOINT/NNP)\n","  SESSION/NNP\n","  OF/IN\n","  (ORGANIZATION THE/NNP)\n","  (ORGANIZATION CONGRESS/NNP)\n","  ON/NNP\n","  THE/NNP\n","  (ORGANIZATION STATE/NNP OF/IN)\n","  (ORGANIZATION THE/NNP)\n","  (ORGANIZATION UNION/NNP)\n","  January/NNP\n","  31/CD\n","  ,/,\n","  2006/CD\n","  (ORGANIZATION THE/NNP)\n","  PRESIDENT/NNP\n","  :/:\n","  Thank/NNP\n","  you/PRP\n","  all/DT\n","  ./.)\n"]}],"source":["def get_named_entity():\n","    for i in tokenized:\n","        words = nltk.word_tokenize(i)\n","        tagged = nltk.pos_tag(words)\n","        namedEnt = nltk.ne_chunk(tagged, binary=False)\n","        print(namedEnt)\n","        namedEnt.draw()\n","        break\n","get_named_entity()"]},{"cell_type":"markdown","metadata":{"id":"PQekGH1_9kDH"},"source":["### 3. Write a program to perform Morphological generation on different surface forms of a word"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680506965859,"user":{"displayName":"E-48 Madhav Jha","userId":"17793726210268974405"},"user_tz":-330},"id":"yriVkoHivw45","outputId":"e2255284-eef2-499b-c0e7-1e9b134d4d35"},"outputs":[{"name":"stdout","output_type":"stream","text":["grace\n","disgrace\n","disgraceful\n","graceful\n","disgraceful\n"]}],"source":["# finite state all possible combinations for root words grace\n","rootWord = \"grace\"\n","prefix = [\"dis\"]\n","suffix = [\"ful\"]\n","\n","def genAll(word, pre, suf, i, j):\n","  print(word)\n","\n","  # pick pre[i]\n","  if(i \u003c len(pre)):\n","    temp = pre[i] + word\n","    genAll(temp, pre, suf, i+1,j)\n","\n","  if(j \u003c len(suf)):\n","    temp = word + suf[j]\n","    genAll(temp, pre, suf, i,j+1)\n","\n","genAll(rootWord, prefix, suffix, 0 , 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RTSW0_vPA-IH"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":0}